diff --git a/PyTorch/LanguageModeling/BERT/modeling.py b/PyTorch/LanguageModeling/BERT/modeling.py
index 27d7e291..b2151eca 100755
--- a/PyTorch/LanguageModeling/BERT/modeling.py
+++ b/PyTorch/LanguageModeling/BERT/modeling.py
@@ -136,8 +136,17 @@ def bias_tanh(bias, y):
 def swish(x):
     return x * torch.sigmoid(x)
 
+def tanh(x):
+    return torch.tanh(x)
+
 #torch.nn.functional.gelu(x) # Breaks ONNX export
-ACT2FN = {"gelu": gelu, "bias_gelu": bias_gelu, "bias_tanh": bias_tanh, "relu": torch.nn.functional.relu, "swish": swish}
+ACT2FN = {"gelu": gelu, "bias_gelu": bias_gelu, "bias_tanh": bias_tanh, "relu": torch.nn.functional.relu, "swish": swish, "tanh": tanh}
+
+try:
+    torch.hpu.is_available()
+    huda_mode = True
+except Exception:
+    huda_mode = False
 
 class LinearActivation(Module):
     r"""Fused Linear and activation Module.
@@ -148,11 +157,12 @@ class LinearActivation(Module):
         super(LinearActivation, self).__init__()
         self.in_features = in_features
         self.out_features = out_features
-        self.act_fn = nn.Identity()                                                         #
+        # setting act_fn to nn.Identity caused issues when re-assigning to gelu.Hence set to None
+        self.act_fn = None
         self.biased_act_fn = None                                                           #
         self.bias = None                                                                    #
         if isinstance(act, str) or (sys.version_info[0] == 2 and isinstance(act, unicode)): # For TorchScript
-            if bias and not 'bias' in act:                                                  # compatibility
+            if bias and not 'bias' in act and not huda_mode:                                # compatibility
                 act = 'bias_' + act                                                         #
                 self.biased_act_fn = ACT2FN[act]                                            #
 
@@ -175,7 +185,7 @@ class LinearActivation(Module):
             init.uniform_(self.bias, -bound, bound)
 
     def forward(self, input):
-        if not self.bias is None:
+        if not self.bias is None and not huda_mode:
             return self.biased_act_fn(self.bias, F.linear(input, self.weight, None))
         else:
             return self.act_fn(F.linear(input, self.weight, self.bias))
diff --git a/PyTorch/LanguageModeling/BERT/run_pretraining.py b/PyTorch/LanguageModeling/BERT/run_pretraining.py
index a3577886..459b1f03 100755
--- a/PyTorch/LanguageModeling/BERT/run_pretraining.py
+++ b/PyTorch/LanguageModeling/BERT/run_pretraining.py
@@ -20,6 +20,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import habana_frameworks.torch.gpu_migration
+import habana_frameworks.torch.core as htcore
 # ==================
 import csv
 import os
@@ -607,10 +609,14 @@ def main():
                     else:
                         loss.backward()
                     average_loss += loss.item()
+                    
+                    htcore.mark_step()
 
                     if training_steps % args.gradient_accumulation_steps == 0:
                         lr_scheduler.step()  # learning rate warmup
                         global_step = take_optimizer_step(args, optimizer, model, overflow_buf, global_step)
+                    
+                    htcore.mark_step()
 
                     if global_step >= args.steps_this_run or timeout_sent:
                         train_time_raw = time.time() - raw_train_start
