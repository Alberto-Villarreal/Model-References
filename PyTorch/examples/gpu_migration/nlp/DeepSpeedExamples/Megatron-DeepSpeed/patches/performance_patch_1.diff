diff --git a/PyTorch/examples/gpu_migration/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/model/transformer.py b/PyTorch/examples/gpu_migration/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/model/transformer.py
index 72641445d..fc6dfa38b 100644
--- a/PyTorch/examples/gpu_migration/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/model/transformer.py
+++ b/PyTorch/examples/gpu_migration/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/model/transformer.py
@@ -69,7 +69,7 @@ class ParallelMLP(MegatronModule):
             args.ffn_hidden_size,
             gather_output=False,
             init_method=init_method,
-            skip_bias_add=True,
+            skip_bias_add=False,
             moe=moe,
             enable_expert_tensor_parallelism=enable_expert_tensor_parallelism
             )
@@ -87,7 +87,7 @@ class ParallelMLP(MegatronModule):
             args.hidden_size,
             input_is_parallel=True,
             init_method=output_layer_init_method,
-            skip_bias_add=True,
+            skip_bias_add=False,
             moe=moe,
             enable_expert_tensor_parallelism=enable_expert_tensor_parallelism)
 
@@ -101,7 +101,7 @@ class ParallelMLP(MegatronModule):
                      bias_gelu_impl(intermediate_parallel, bias_parallel)
         else:
             intermediate_parallel = \
-                self.activation_func(intermediate_parallel + bias_parallel)
+                self.activation_func(intermediate_parallel)
 
         # [s, b, h]
         output, output_bias = self.dense_4h_to_h(intermediate_parallel)
@@ -188,7 +188,7 @@ class ParallelAttention(MegatronModule):
             args.hidden_size,
             input_is_parallel=True,
             init_method=output_layer_init_method,
-            skip_bias_add=True)
+            skip_bias_add=False)
 
         if deepspeed.checkpointing.is_configured():
             global get_cuda_rng_tracker, checkpoint
@@ -366,7 +366,8 @@ class ParallelAttention(MegatronModule):
 
 def bias_dropout_add(x, bias, residual, prob, training):
     # type: (Tensor, Tensor, Tensor, float, bool) -> Tensor
-    out = torch.nn.functional.dropout(x + bias, p=prob, training=training)
+    x = x + bias if bias is not None else x
+    out = torch.nn.functional.dropout(x, p=prob, training=training)
     out = residual + out
     return out
 
@@ -503,7 +504,7 @@ class ParallelTransformerLayer(MegatronModule):
         with torch.enable_grad():
             layernorm_input = bias_dropout_add_func(
                 attention_output,
-                attention_bias.expand_as(residual),
+                attention_bias.expand_as(residual) if attention_bias is not None else None,
                 residual,
                 self.hidden_dropout)
 
@@ -525,7 +526,7 @@ class ParallelTransformerLayer(MegatronModule):
             with torch.enable_grad():
                 layernorm_input = bias_dropout_add_func(
                     attention_output,
-                    attention_bias.expand_as(residual),
+                    attention_bias.expand_as(residual) if attention_bias is not None else None,
                     residual,
                     self.hidden_dropout)
 
@@ -552,7 +553,7 @@ class ParallelTransformerLayer(MegatronModule):
             #if self.num_experts <= 1:
             output = bias_dropout_add_func(
                     mlp_output,
-                    mlp_bias.expand_as(residual),
+                    mlp_bias.expand_as(residual) if mlp_bias is not None else None,
                     residual,
                     self.hidden_dropout)
             #else:
