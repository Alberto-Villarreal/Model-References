diff --git a/references/classification/train.py b/references/classification/train.py
--- a/references/classification/train.py
+++ b/references/classification/train.py
@@ -194,6 +194,17 @@
 
     return dataset, dataset_test, train_sampler, test_sampler
 
+def lr_vec_fcn(values, milestones):
+    lr_vec = []
+    for n in range(len(milestones)-1):
+        lr_vec += [values[n]]*(milestones[n+1]-milestones[n])
+    return lr_vec
+
+def adjust_learning_rate(optimizer, epoch, lr_vec):
+    """Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""
+    lr = lr_vec[epoch]
+    for param_group in optimizer.param_groups:
+        param_group['lr'] = lr
 
 def main(args):
     if args.output_dir:
@@ -201,6 +212,8 @@
 
     utils.init_distributed_mode(args)
     print(args)
+
+    torch.manual_seed(args.seed)
 
     device = torch.device(args.device)
 
@@ -297,6 +310,11 @@
         )
     elif args.lr_scheduler == "exponentiallr":
         main_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=args.lr_gamma)
+    elif args.lr_scheduler == "custom_lr":
+        costom_lr_values = [0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008]
+        custom_lr_milestones = [1, 2, 3, 4, 30, 60, 80]
+        lr_vec = lr_vec_fcn([args.lr]+ costom_lr_values, [0]+custom_lr_milestones+[args.epochs])
+        main_lr_scheduler = None
     else:
         raise RuntimeError(
             f"Invalid lr scheduler '{args.lr_scheduler}'. Only StepLR, CosineAnnealingLR and ExponentialLR "
@@ -367,8 +385,15 @@
     for epoch in range(args.start_epoch, args.epochs):
         if args.distributed and args.dl_worker_type != "HABANA":
             train_sampler.set_epoch(epoch)
+
+        if args.lr_scheduler == "custom_lr":
+            adjust_learning_rate(optimizer, epoch, lr_vec)
+
         train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, args, model_ema, scaler)
-        lr_scheduler.step()
+
+        if not args.lr_scheduler == "custom_lr":
+            lr_scheduler.step()
+
         evaluate(model, criterion, data_loader_test, device=device)
         if model_ema:
             evaluate(model_ema, criterion, data_loader_test, device=device, log_suffix="EMA")
@@ -376,7 +401,7 @@
             checkpoint = {
                 "model": model_without_ddp.state_dict(),
                 "optimizer": optimizer.state_dict(),
-                "lr_scheduler": lr_scheduler.state_dict(),
+                "lr_scheduler": lr_scheduler.state_dict() if lr_scheduler is not None else lr_scheduler,
                 "epoch": epoch,
                 "args": args,
             }
@@ -444,7 +469,7 @@
     )
     parser.add_argument("--mixup-alpha", default=0.0, type=float, help="mixup alpha (default: 0.0)")
     parser.add_argument("--cutmix-alpha", default=0.0, type=float, help="cutmix alpha (default: 0.0)")
-    parser.add_argument("--lr-scheduler", default="steplr", type=str, help="the lr scheduler (default: steplr)")
+    parser.add_argument("--lr-scheduler", default="custom_lr", type=str, help="the lr scheduler (default: custom_lr)")
     parser.add_argument("--lr-warmup-epochs", default=0, type=int, help="the number of epochs to warmup (default: 0)")
     parser.add_argument(
         "--lr-warmup-method", default="constant", type=str, help="the warmup method (default: constant)"
@@ -457,6 +482,7 @@
     parser.add_argument("--output-dir", default=".", type=str, help="path to save outputs")
     parser.add_argument("--resume", default="", type=str, help="path of checkpoint")
     parser.add_argument("--start-epoch", default=0, type=int, metavar="N", help="start epoch")
+    parser.add_argument('--seed', type=int, default=123, help='random seed')
     parser.add_argument(
         "--cache-dataset",
         dest="cache_dataset",

